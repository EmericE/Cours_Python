{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import used packages\n",
    "import numpy as np\n",
    "import pickle # save and load binary files (data, model)\n",
    "from gradient import GradientDescent\n",
    "from gradient.example import features, labels\n",
    "\n",
    "# Set jupyter display in full screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientDescent in module gradient.gradient_descent:\n",
      "\n",
      "class GradientDescent(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, regularize=True, bias=True, alpha=3e-09)\n",
      " |      Descent gradient class with regularize technique\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      regularize : bool\n",
      " |          If True, the regularization is used.\n",
      " |      bias : bool\n",
      " |          If the True, a bias is added to the features.\n",
      " |      alpha : float > 0\n",
      " |          Coefficient for the step when updating the parameters.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This class aims at computing the parameters of a linear model using\n",
      " |      a descent gradient method with or without regularization.\n",
      " |  \n",
      " |  costFunction(self, yhat, y)\n",
      " |      Fonction de coût\n",
      " |  \n",
      " |  fit(self, features, label, parameters=None)\n",
      " |      Find the optimal parameters\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features : 2d sequence of float\n",
      " |          The input parameters.\n",
      " |      label : 2d sequence of float\n",
      " |          The output parameters\n",
      " |      parameters : 2d sequence of float\n",
      " |          The initial guess for the descent gradient.\n",
      " |  \n",
      " |  gradients(self, yhat, y, x)\n",
      " |      Dérivée de la fonction de coût == gradients\n",
      " |  \n",
      " |  hypothesis(self, x, theta)\n",
      " |      Compute our hypothesis model (linear regression), use a fonction:\n",
      " |  \n",
      " |  predict(self, new_features)\n",
      " |      Make predictions using the result of the gradient descent\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_features : 2d sequence of float\n",
      " |          The feature for which to predict the labels.\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_labels : 2d sequence of float\n",
      " |          The predicted labels\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The method fit must be called first.\n",
      " |  \n",
      " |  regCostFunction(self, yhat, y, lmb, theta)\n",
      " |      Fonction de coût régularisée\n",
      " |  \n",
      " |  regGradients(self, yhat, y, x, lmb, theta)\n",
      " |      Dérivée de la fonction de coût regularisée\n",
      " |  \n",
      " |  testCostFct(self, yhat, y, prevCostFct, epsilon)\n",
      " |      Fonction pour tester l'évolution de la fonction de coût: vrai = continuer la descente de gradient\n",
      " |  \n",
      " |  testRegCostFct(self, yhat, y, lmb, theta, prevCostFct, epsilon)\n",
      " |      Fonction pour tester l'évolution de la fonction de coût régularisée\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      test : bool\n",
      " |          vrai = continuer la descente de gradient\n",
      " |  \n",
      " |  updateParameters(self, parameters, grads, alpha)\n",
      " |      Gradient descent: mise à jour des paramètres\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientDescent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 : cost function = 6966.837217779474\n",
      "2000 : cost function = 5662.245310833882\n",
      "3000 : cost function = 4732.189044632663\n",
      "4000 : cost function = 4066.4116770113033\n",
      "5000 : cost function = 3589.329565162859\n",
      "6000 : cost function = 3247.242376351706\n",
      "7000 : cost function = 3001.767344399758\n",
      "8000 : cost function = 2825.4428139991915\n",
      "9000 : cost function = 2698.617509128693\n",
      "10000 : cost function = 2607.2286579058205\n",
      "11000 : cost function = 2541.2125240103164\n",
      "12000 : cost function = 2493.3670547793367\n",
      "13000 : cost function = 2458.5380094764746\n",
      "\n",
      "Finish: 13627 steps, cost function = 2441.6442789346956\n"
     ]
    }
   ],
   "source": [
    "# initialize l'algorithme\n",
    "grad_descent_classic = GradientDescent(regularize=False)\n",
    "# exécute l'algorithme pour calculer les paramètres\n",
    "grad_descent_classic.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.728103198795404],\n",
       "       [9.403694571352597],\n",
       "       [-5.700951897983738],\n",
       "       [10.144156096850361],\n",
       "       [-6.698470706930141],\n",
       "       [46.104102994375964],\n",
       "       [-6.698470706930141],\n",
       "       [11.632733335126986],\n",
       "       [12.107437089425755],\n",
       "       [11.130840302111602]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faire des prédictions\n",
    "grad_descent_classic.predict(features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 : cost function = 6749913.906355833\n",
      " 20 : cost function = 180543.0943570395\n",
      " 30 : cost function = 106239.3404803313\n",
      " 40 : cost function = 105385.78071912074\n",
      " 50 : cost function = 105375.95773993517\n",
      " 60 : cost function = 105375.84465598175\n",
      "\n",
      "Finish: 69 steps, cost function = 105375.84336245203\n"
     ]
    }
   ],
   "source": [
    "# idem avec la régularisation\n",
    "grad_descent_regularize = GradientDescent()\n",
    "solution_reg = grad_descent_regularize.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
